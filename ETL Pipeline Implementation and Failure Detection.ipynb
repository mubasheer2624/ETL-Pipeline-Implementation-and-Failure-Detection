{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24adff36",
   "metadata": {},
   "source": [
    "# Spark ETL Pipeline Implementation and Failure Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7b910",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6b9ec",
   "metadata": {},
   "source": [
    "We will commence by extracting the required datasets for this project from various CSV files. The datasets include the `Pipeline data table`, `Maintenance data table`, `Sensor data table`, and `Inspection data table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36742505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/Mubashir/spark')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76f6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13c9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting path for postgres driver\n",
    "import os\n",
    "os.environ['PYSPARK_ARGS'] = '--driver-class-path /Users/Mubashir/postgresql-42.6.0.jar pyspark-shell'\n",
    "\n",
    "# this sets a path for the postgres driver. It will be needed in the loading section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b5cee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/21 12:48:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a SparkSession which serves as a entry point for interacting with Spark\n",
    "spark = SparkSession.builder.appName(\"ETL Data Pipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc800819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Reading the csv files\n",
    "pipeline_df = spark.read.csv('pipeline_data_table.csv', header = True, inferSchema = True)\n",
    "maintenance_df = spark.read.csv('maintenance_data_table.csv', header = True, inferSchema = True)\n",
    "sensor_df = spark.read.csv('sensor_data_table.csv', header = True, inferSchema = True)\n",
    "inspection_df = spark.read.csv('inspection_data_table.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c04c2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------------+------------------+---------+--------+------------+\n",
      "|_c0|pipeline_id|        lenght_km|     diamteters_mm|age_years|material|    location|\n",
      "+---+-----------+-----------------+------------------+---------+--------+------------+\n",
      "|  0|       PL-1|3.163867170339976|205.96414132846047|       39|   steel|       lagos|\n",
      "|  1|       PL-2|15.12457355971772| 655.0772854593554|       27| plastic|portharcourt|\n",
      "|  2|       PL-3|43.00378167829604|237.04409028519854|       32|   steel|portharcourt|\n",
      "|  3|       PL-4|78.16478032119788| 611.5345293219675|       19|concrete|       lagos|\n",
      "|  4|       PL-5|76.14520496673097| 603.4796154760029|       41|concrete|       lagos|\n",
      "+---+-----------+-----------------+------------------+---------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 12:48:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , pipeline_id, lenght_km, diamteters_mm, age_years, material, location\n",
      " Schema: _c0, pipeline_id, lenght_km, diamteters_mm, age_years, material, location\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/Judith/pipeline_data_table.csv\n"
     ]
    }
   ],
   "source": [
    "#Checking the head of our dat\n",
    "pipeline_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba4ec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+---------------------+-----------------+\n",
      "|_c0|pipeline_id|repair_type|repair_duration_hours|  cost_in_dollars|\n",
      "+---+-----------+-----------+---------------------+-----------------+\n",
      "|  0|       PL-1| preventive|                    8|486.9355980554817|\n",
      "|  1|       PL-2|    routine|                    2|            313.0|\n",
      "|  2|       PL-3|    routine|                   11|            252.0|\n",
      "|  3|       PL-4| preventive|                   23|784.6322648503146|\n",
      "|  4|       PL-5| corrective|                    9|11137.27992806314|\n",
      "+---+-----------+-----------+---------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 12:48:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , pipeline_id, repair_type, repair_duration_hours, cost_in_dollars\n",
      " Schema: _c0, pipeline_id, repair_type, repair_duration_hours, cost_in_dollars\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/Judith/maintenance_data_table.csv\n"
     ]
    }
   ],
   "source": [
    "maintenance_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51357e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+----------+------------------+------------------+------------------+\n",
      "|_c0|sensor_id|pipeline_id|      date|          pressure|       temperature|         flow_rate|\n",
      "+---+---------+-----------+----------+------------------+------------------+------------------+\n",
      "|  0| sensor-2|       PL-1|2016-01-23| 89.16509363947836|26.487407250471655| 8.841277524541173|\n",
      "|  1| sensor-4|       PL-2|2021-05-06|51.422621452961415|22.123033999440032|2.2398605213687572|\n",
      "|  2| sensor-1|       PL-3|2014-08-03| 89.93827002882762|24.376792602298494| 8.798215574407859|\n",
      "|  3| sensor-2|       PL-4|2019-03-26|  91.6988260369984| 24.12896689486083|4.4345016457065976|\n",
      "|  4| sensor-3|       PL-5|2015-01-19| 77.33094021729232|23.286447126274545| 4.723534211586986|\n",
      "+---+---------+-----------+----------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 12:48:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , sensor_id, pipeline_id, date, pressure, temperature, flow_rate\n",
      " Schema: _c0, sensor_id, pipeline_id, date, pressure, temperature, flow_rate\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/Judith/sensor_data_table.csv\n"
     ]
    }
   ],
   "source": [
    "sensor_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e692a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------------+-----------------+--------------+\n",
      "|_c0|pipeline_id|corrosion_level|deformation_level|leak_detection|\n",
      "+---+-----------+---------------+-----------------+--------------+\n",
      "|  0|       PL-1|              4|                2|           Yes|\n",
      "|  1|       PL-2|              3|                3|            No|\n",
      "|  2|       PL-3|              1|                0|            No|\n",
      "|  3|       PL-4|              2|                4|           Yes|\n",
      "|  4|       PL-5|              2|                3|            No|\n",
      "+---+-----------+---------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 12:48:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , pipeline_id, corrosion_level, deformation_level, leak_detection\n",
      " Schema: _c0, pipeline_id, corrosion_level, deformation_level, leak_detection\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/Judith/inspection_data_table.csv\n"
     ]
    }
   ],
   "source": [
    "inspection_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787bfbde",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68396d72",
   "metadata": {},
   "source": [
    "The main goal of this part is to select the important columns from each dataset and combine them into a single dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab113b5c",
   "metadata": {},
   "source": [
    "We will add a newÂ column called \"pipeline status\" that will be crucial. Binary values of 1 and 0 will be displayed in the \"pipeline status\" column. A pipeline failure is represented by a number of 1, while no failures are present and the value is 0. We will use particular conditions generated from the existing columns to find these values, enabling us to precisely identify and trace pipeline faults throughout the data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531df784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting important columns from pipeline_df dataset\n",
    "pipeline_columns = ['pipeline_id','lenght_km', 'diamteters_mm', 'age_years', 'material', 'location']\n",
    "pipeline_selected_df = pipeline_df.select(pipeline_columns)\n",
    "\n",
    "# Selecting important columns from maintenance dataset\n",
    "maintenance_columns = ['pipeline_id', 'repair_type', 'repair_duration_hours']\n",
    "maintenance_selected_df = maintenance_df.select(maintenance_columns)\n",
    "\n",
    "# Selecting important columns from sensor dataset\n",
    "sensor_columns = ['pipeline_id', 'pressure', 'temperature', 'flow_rate']\n",
    "sensor_selected_df = sensor_df.select(sensor_columns)\n",
    "\n",
    "# Selecting important columns from inspection dataset\n",
    "inspection_columns = ['pipeline_id','corrosion_level', 'deformation_level', 'leak_detection']\n",
    "inspection_selected_df = inspection_df.select(inspection_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f049ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset that is made up of selected columns from existing datasets, and joining them based on their \n",
    "# primary key ('pipeline_id')\n",
    "joined_df = pipeline_selected_df \\\n",
    "    .join(maintenance_selected_df, \"pipeline_id\", \"left\") \\\n",
    "    .join(sensor_selected_df, \"pipeline_id\", \"left\") \\\n",
    "    .join(inspection_selected_df, \"pipeline_id\", \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf55746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+\n",
      "|pipeline_id|        lenght_km|     diamteters_mm|age_years|material|    location|repair_type|repair_duration_hours|          pressure|       temperature|         flow_rate|corrosion_level|deformation_level|leak_detection|\n",
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+\n",
      "|       PL-1|3.163867170339976|205.96414132846047|       39|   steel|       lagos| preventive|                    8| 89.16509363947836|26.487407250471655| 8.841277524541173|              4|                2|           Yes|\n",
      "|       PL-2|15.12457355971772| 655.0772854593554|       27| plastic|portharcourt|    routine|                    2|51.422621452961415|22.123033999440032|2.2398605213687572|              3|                3|            No|\n",
      "|       PL-3|43.00378167829604|237.04409028519854|       32|   steel|portharcourt|    routine|                   11| 89.93827002882762|24.376792602298494| 8.798215574407859|              1|                0|            No|\n",
      "|       PL-4|78.16478032119788| 611.5345293219675|       19|concrete|       lagos| preventive|                   23|  91.6988260369984| 24.12896689486083|4.4345016457065976|              2|                4|           Yes|\n",
      "|       PL-5|76.14520496673097| 603.4796154760029|       41|concrete|       lagos| corrective|                    9| 77.33094021729232|23.286447126274545| 4.723534211586986|              2|                3|            No|\n",
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the resulted dataframe\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477aa65",
   "metadata": {},
   "source": [
    "> ### We will create a new column called `pipeline_status` that is based on certain conditions derived from some existing columns in the unified joined_df dataset\n",
    "\n",
    "The selected columns used to determine the values for the `pipeline_status` column are `corrosion_level`, `deformation_level`, `leak_detection`, `age_years`, and `repair_type`. These columns were selected becasue they are the most likely to have an effect on the wear and tear of the pipleline.\n",
    "\n",
    "Ranks will be assigned to the values in these selected columns, with the highest possible sum of rank being 20. If the sum of these rank values for a particular row exceeds 10, the `pipeline_status` value for that row will be set to 1, indicating pipeline failure. On the other hand, if the sum of the ranks for each row is less than or equal to 10, the `pipeline_status` value for that row will be set to 0, indicating no pipeline failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7eaa0",
   "metadata": {},
   "source": [
    "### 1. Creating new columns that ranks each of the selected  columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edf19fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.withColumn('corrosion_rank', \n",
    "                                 when(col('corrosion_level') == 0, 0).\n",
    "                                 when(col('corrosion_level') == 1, 1).\n",
    "                                 when(col('corrosion_level') == 2, 2).\n",
    "                                 when(col('corrosion_level') == 3, 3).\n",
    "                                 when(col('corrosion_level') == 4, 4).\n",
    "                                 when(col('corrosion_level') == 5, 5))\n",
    "\n",
    "joined_df = joined_df.withColumn('deformation_rank', \n",
    "                                 when(col('deformation_level') == 0, 0).\n",
    "                                 when(col('deformation_level') == 1, 1).\n",
    "                                 when(col('deformation_level') == 2, 2).\n",
    "                                 when(col('deformation_level') == 3, 3).\n",
    "                                 when(col('deformation_level') == 4, 4).\n",
    "                                 when(col('deformation_level') == 5, 5))\n",
    "\n",
    "joined_df = joined_df.withColumn('leak_detection_rank', \n",
    "                                 when(col('leak_detection') == 'Yes', 2).\n",
    "                                 when(col('leak_detection') == 'No', 1))\n",
    "                                 \n",
    "joined_df = joined_df.withColumn('age_years_rank', \n",
    "                                 when(col('age_years') <= 10, 1).\n",
    "                                 when(col('age_years') <= 20, 2).\n",
    "                                 when(col('age_years') <= 30, 3).\n",
    "                                 when(col('age_years') <= 40, 4).\n",
    "                                 when(col('age_years') <= 50, 5))\n",
    "\n",
    "joined_df = joined_df.withColumn('repair_type_rank', \n",
    "                                 when(col('repair_type') == 'routine', 1).\n",
    "                                 when(col('repair_type') == 'preventive', 2).\n",
    "                                 when(col('repair_type') == 'corrective', 3))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224a7e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|age_years|age_years_rank|\n",
      "+---------+--------------+\n",
      "|       39|             4|\n",
      "|       27|             3|\n",
      "|       32|             4|\n",
      "|       19|             2|\n",
      "|       41|             5|\n",
      "+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing our rank columns to see if the mapping worked properly\n",
    "\n",
    "joined_df.select(\"age_years\", \"age_years_rank\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d4909",
   "metadata": {},
   "source": [
    "### 2. Creating a column that results the sum of all the ranked columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae3d8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.withColumn('summed_ranks', col('corrosion_rank') + col('deformation_rank') \n",
    "                                 + col('leak_detection_rank') + col('age_years_rank') + col('repair_type_rank'))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9708e565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------------------+--------------+----------------+------------+\n",
      "|corrosion_rank|deformation_rank|leak_detection_rank|age_years_rank|repair_type_rank|summed_ranks|\n",
      "+--------------+----------------+-------------------+--------------+----------------+------------+\n",
      "|             4|               2|                  2|             4|               2|          14|\n",
      "|             3|               3|                  1|             3|               1|          11|\n",
      "|             1|               0|                  1|             4|               1|           7|\n",
      "|             2|               4|                  2|             2|               2|          12|\n",
      "|             2|               3|                  1|             5|               3|          14|\n",
      "+--------------+----------------+-------------------+--------------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the result\n",
    "joined_df.select('corrosion_rank','deformation_rank','leak_detection_rank',\n",
    "                 'age_years_rank','repair_type_rank','summed_ranks').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea893d5",
   "metadata": {},
   "source": [
    "### 3. Creating a `pipeline_status` column that result 1 if the summed_ranks column is greater than 10 and results 0 if the summed_ranks column is less than 10.\n",
    "\n",
    "In the pipeline_status, \n",
    "\n",
    "- 1 indicates pipeline failure\n",
    "- 0 indicates no pipeline failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c63bdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.withColumn('pipeline_status', when(col('summed_ranks')>10, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6885fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|pipeline_status|\n",
      "+---------------+\n",
      "|              1|\n",
      "|              1|\n",
      "|              0|\n",
      "|              1|\n",
      "|              1|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the result\n",
    "joined_df.select('pipeline_status').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "622884ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping irrelevant columns\n",
    "columns_to_drop = ['corrosion_rank', 'deformation_rank', 'leak_detection_rank', 'age_years_rank',\n",
    "                   'repair_type_rank', 'summed_ranks']\n",
    "\n",
    "joined_df = joined_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5ef6c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+---------------+\n",
      "|pipeline_id|        lenght_km|     diamteters_mm|age_years|material|    location|repair_type|repair_duration_hours|          pressure|       temperature|         flow_rate|corrosion_level|deformation_level|leak_detection|pipeline_status|\n",
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+---------------+\n",
      "|       PL-1|3.163867170339976|205.96414132846047|       39|   steel|       lagos| preventive|                    8| 89.16509363947836|26.487407250471655| 8.841277524541173|              4|                2|           Yes|              1|\n",
      "|       PL-2|15.12457355971772| 655.0772854593554|       27| plastic|portharcourt|    routine|                    2|51.422621452961415|22.123033999440032|2.2398605213687572|              3|                3|            No|              1|\n",
      "|       PL-3|43.00378167829604|237.04409028519854|       32|   steel|portharcourt|    routine|                   11| 89.93827002882762|24.376792602298494| 8.798215574407859|              1|                0|            No|              0|\n",
      "|       PL-4|78.16478032119788| 611.5345293219675|       19|concrete|       lagos| preventive|                   23|  91.6988260369984| 24.12896689486083|4.4345016457065976|              2|                4|           Yes|              1|\n",
      "|       PL-5|76.14520496673097| 603.4796154760029|       41|concrete|       lagos| corrective|                    9| 77.33094021729232|23.286447126274545| 4.723534211586986|              2|                3|            No|              1|\n",
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the result\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d512e",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19dfa0",
   "metadata": {},
   "source": [
    "Our finalised dataset will be loaded into the Postgres Database. \n",
    "\n",
    "This procedure ensures secure storage and makes it easier to use the processed data for later analysis, reports, and decision-making processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2afc9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in postgres connection details\n",
    "postgres_url = \"jdbc:postgresql://localhost:5432/new_database\"\n",
    "properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"reallyStrongPwd123\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "028fdc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_df.write \\\n",
    "    .jdbc(url=postgres_url, table=\"pipeline_failure\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5753a",
   "metadata": {},
   "source": [
    "After successfully loading our database into Postgres, we can now query it and carry out additional analysis there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
